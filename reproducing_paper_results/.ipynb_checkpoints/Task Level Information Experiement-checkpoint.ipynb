{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 30\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpermutil\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpu\u001b[39;00m\n\u001b[1;32m     29\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../..\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'utils'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "import itertools\n",
    "import os\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "import pandas as pd\n",
    "warnings.simplefilter(action='ignore', category=pd.errors.ParserWarning)\n",
    "\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import copy\n",
    "import pandas as pd\n",
    "#import seaborn as sns\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "import scipy\n",
    "import sys\n",
    "sys.path.append('top-k-mallows/')\n",
    "import permutil as pu\n",
    "sys.path.insert(0,'../..')\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Each version of the dataset in the form of :\n",
    "\n",
    "$$Score_1 = \\begin{bmatrix} &  T_{1} & \\cdots & T_{|T|}\\\\\n",
    "S_1 & \\cdots & \\cdots & \\cdots \\\\\n",
    "S_2 & \\cdots &\\cdots & \\cdots \\\\\n",
    "\\cdots &\\cdots &\\cdots& \\cdots \\\\\n",
    "S_{|S|} & \\cdots& \\cdots& \\cdots  \\\\\n",
    "\\end{bmatrix}$$, \n",
    "\n",
    "where each score of the task is a column of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently I have collected data for [GLUE](https://openreview.net/pdf?id=rJ4km2R5t7) , [SUPERGLUE](https://arxiv.org/abs/1905.00537) , [XTREME](https://arxiv.org/pdf/2104.07412.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASETS = ['xtrem','glue','superglue'] # available datasets\n",
    "number = 0 # which dataset to load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "count = 0\n",
    "for ds in  DATASETS:\n",
    "    print('Loading',ds)\n",
    "    df =  pd.read_csv('data/{}.data'.format(ds),index_col=False)\n",
    "    df =  pd.read_csv('data/{}.data'.format(ds),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1]) #filter columns\n",
    "    count +=  df.shape[0]* df.shape[1]\n",
    "    print('Shape',df.shape)\n",
    "print('Number of points',count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('Loading', DATASETS[number])\n",
    "df =  pd.read_csv('data/{}.data'.format(DATASETS[number]),index_col=False)\n",
    "df =  pd.read_csv('data/{}.data'.format(DATASETS[number]),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1]) #filter columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#my guess for this function but the values don't match with the old file\n",
    "distorsion_measure =  lambda x, y : sum([pu.distance(x,i) for i in y])/len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankings = []\n",
    "for index,columns in df.iteritems():\n",
    "    if index != 'Model':\n",
    "        rankings.append(columns.argsort().argsort().values.tolist())\n",
    "\n",
    "ranking_borda = ranking_aggregation(df).tolist()\n",
    "print('Borda Distorsion', distorsion_measure(ranking_borda,rankings)) #the old file printed 12.25\n",
    "#print(np.array(rankings))\n",
    "#print(np.array(ranking_borda))\n",
    "ranking_mean = mean_aggregation_task_level(df)[0].tolist()\n",
    "print('Mean Distorsion', distorsion_measure(ranking_mean,rankings)) #the old file printed 12.75\n",
    "#print(ranking_mean)\n",
    "random_correlation_measures = []\n",
    "list_to_permute = list(range(len(ranking_mean)))\n",
    "for i in tqdm(range(100)): # generate 100 random permutations\n",
    "    random.shuffle(list_to_permute)\n",
    "    random_correlation_measures.append( distorsion_measure(list_to_permute,rankings))\n",
    "\n",
    "print('Mean Random Distorsion', sum(random_correlation_measures)/len(random_correlation_measures))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE_RANKING = False\n",
    "if VISUALIZE_RANKING:\n",
    "    df = df.set_index('Model') # display data\n",
    "    mean_ranking,means = mean_aggregation_task_level(df)\n",
    "    print('Ranking Mean',mean_ranking)\n",
    "    print('Means Scores',means)\n",
    "    ranking_borda,borda_count = ranking_aggregation(df,True)\n",
    "    print('Ranking Borda',ranking_borda)\n",
    "    print('Borda Count',borda_count)\n",
    "    df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyse the ranking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ranking,means = mean_aggregation_task_level(df)\n",
    "#mean_reindex = reverse_ranking(mean_ranking)[::-1] \n",
    "mean_reindex = np.argsort(-np.array(means))\n",
    "print('Ranking mean',mean_ranking)\n",
    "print('Means',means)\n",
    "print('ranking_borda_reindex',mean_reindex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df.reindex(mean_reindex,copy = False)\n",
    "df_new.reset_index(inplace=True)\n",
    "del df_new['index']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df_new.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranking_borda,borda_count = ranking_aggregation(df_new,True)\n",
    "#ranking_borda_reindex = reverse_ranking(ranking_borda)[::-1]\n",
    "ranking_borda_reindex = np.argsort(-np.array(ranking_borda))\n",
    "print('Ranking Borda',ranking_borda)\n",
    "print('Count Borda',borda_count)\n",
    "print('ranking_borda_reindex',ranking_borda_reindex)\n",
    "print('Sorted Borda Ciount',sorted(borda_count)[::-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.reindex(ranking_borda_reindex).head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new.reindex(ranking_borda_reindex).mean(axis=1,numeric_only=True).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robutness experiement when removing metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_metric_experiments_benchmark(number_of_samples,ds,seed=42):\n",
    "    df =  pd.read_csv('data/{}.data'.format(ds),index_col=False)\n",
    "    #df.dropna(inplace=True,axis=1)\n",
    "    considered_df =  pd.read_csv('data/{}.data'.format(ds),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1])\n",
    "    considered_df =considered_df.set_index(['Model'])\n",
    "    print('columns',considered_df.columns)\n",
    "    considered_columns = list(considered_df.columns)\n",
    "    random.seed(seed)\n",
    "    final_dic = {\n",
    "        'mean':{},\n",
    "        'ranking':{}\n",
    "    }\n",
    "    for i in range(1,len(considered_columns)+1):\n",
    "        final_dic['mean']['{}'.format(i)] = []\n",
    "        final_dic['ranking']['{}'.format(i)] = []\n",
    "\n",
    "    for _ in tqdm(range(number_of_samples),'Nbs of Experiments'):\n",
    "        random.shuffle(considered_columns)\n",
    "        for i in range(1,len(considered_columns)+1):\n",
    "            running_df =  considered_df[considered_columns[:i]]\n",
    "            c_mean= mean_aggregation_task_level(running_df)[0]\n",
    "            c_ranking = ranking_aggregation(running_df)\n",
    "            final_dic['mean']['{}'.format(i)].append(c_mean)\n",
    "            final_dic['ranking']['{}'.format(i)].append(c_ranking)\n",
    "    return final_dic,len(considered_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_results_experiments_benchmark(final_dic,ds,len_norm):\n",
    "    df =  pd.read_csv('data/{}.data'.format(ds),index_col=False)\n",
    "    considered_df =  pd.read_csv('data/{}.data'.format(ds),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1])\n",
    "    print('columns',considered_df.columns)\n",
    "    considered_df =considered_df.set_index(['Model'])\n",
    "    \n",
    "    scores_to_plot = {\n",
    "    'criterion': [],\n",
    "    'method': [],\n",
    "    'correlation': [],\n",
    "        'kendallTau': [],\n",
    "    }\n",
    "    ref_ranks = {\n",
    "         'mean': mean_aggregation_task_level(considered_df)[0],\n",
    "        'ranking':  ranking_aggregation(considered_df)\n",
    "    }\n",
    "    for k,v in final_dic.items(): \n",
    "        ref_rank = ref_ranks[k]\n",
    "        for sub_k,sub_v in tqdm(v.items(),'Itterations'):\n",
    "            for score in sub_v:\n",
    "                scores_to_plot['criterion'].append(int(sub_k) / (len_norm) )\n",
    "                scores_to_plot['method'].append(k)\n",
    "                scores_to_plot['correlation'].append(stats.kendalltau(score, ref_rank)[0])\n",
    "                scores_to_plot['kendallTau'].append(stats.kendalltau(score, ref_rank))\n",
    "    return scores_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_DATA_ROBUTNESS_METRIC  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if GENERATE_DATA_ROBUTNESS_METRIC:    \n",
    "    seed = 40\n",
    "    number_of_samples = 1000\n",
    "    for ds in DATASETS:\n",
    "        print(ds)\n",
    "        final_dic,len_norm = generate_metric_experiments_benchmark(number_of_samples,ds,seed=42)\n",
    "        json_results = compute_results_experiments_benchmark(final_dic,ds,len_norm)\n",
    "        with open('result_benchmark_metric_{}_{}.json'.format(ds,number_of_samples),'w') as file:\n",
    "            json.dump(json_results,file)\n",
    "\n",
    "        plt.figure(figsize=(12,8))\n",
    "        df_to_plot = pd.DataFrame(json_results)\n",
    "        sns.lineplot(x=\"criterion\", y=\"correlation\", hue=\"method\",data=df_to_plot)\n",
    "        plt.title('{} {}'.format(ds,number_of_samples))\n",
    "        plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "considered_ds = DATASETS[0]\n",
    "print(considered_ds)\n",
    "assert considered_ds in DATASETS\n",
    "number_of_samples = 1000   \n",
    "with open('result_benchmark_metric_{}_{}.json'.format(considered_ds,number_of_samples),'r') as file:\n",
    "        json_results = json.load(file)\n",
    "#print(json_results)\n",
    "df_to_plot = pd.DataFrame(json_results)\n",
    "#print(df_to_plot)\n",
    "sns.set_palette(\"Set2\")\n",
    "fig,ax = plt.subplots(figsize=(6,6))\n",
    "sns.lineplot(x=\"criterion\", y=\"correlation\", hue=\"method\",style=\"method\",data=df_to_plot,linewidth = 5,ci=100)\n",
    "plt.yticks(fontsize=20) #hue=\"losses\",\n",
    "plt.xticks(fontsize=20) #hue=\"losses\",\n",
    "    #plt.title('{}'.format(title),fontsize=30)\n",
    "plt.ylabel('$\\\\tau$',fontsize=30)\n",
    "plt.xlabel('# Tasks (%)',fontsize=25)\n",
    "plt.tight_layout()\n",
    "L=plt.legend(fontsize=23)\n",
    "for line in L.get_lines():\n",
    "    line.set_linewidth(5.0)\n",
    "#L.get_texts()[0].set_text('Aggreg.')\n",
    "L.get_texts()[0].set_text('$\\\\sigma^{mean}$')\n",
    "L.get_texts()[1].set_text('$\\\\sigma^{*}$')\n",
    "\n",
    "N = 6\n",
    "ax.set_yticks(np.round(np.linspace(0.76, 1, N), 2))\n",
    "\n",
    "N = 5\n",
    "ax.set_xticks(np.round(np.linspace(0.25 if 'xtrem' in considered_ds else 0, 1, N), 2))\n",
    "plt.savefig('one_level_ranking_{}.pdf'.format(considered_ds),format='pdf')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Robutness experiement when adding systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_system_experiment_benchmark(number_of_samples,path= 'final_df',ds='xtrem',seed=42):\n",
    "    random.seed(seed)\n",
    "    l_final_dic = []\n",
    "    df =  pd.read_csv('data/{}.data'.format(ds),index_col=False)\n",
    "    data_df =  pd.read_csv('data/{}.data'.format(ds),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1])\n",
    "    data_df =data_df.set_index(['Model'])\n",
    "    number_of_systems = len(copy.deepcopy(list(set(data_df.reset_index().Model.values)))) +1\n",
    "    for number_of_systems in tqdm(range(1,number_of_systems + 1),'Number of Systems'): # start at 2\n",
    "        for _ in tqdm(range(number_of_samples),'Experiments'):\n",
    "            df =  pd.read_csv('data/{}.data'.format(ds),index_col=False)\n",
    "            data_df =  pd.read_csv('data/{}.data'.format(ds),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1])\n",
    "            data_df =data_df.set_index(['Model'])\n",
    "            systems = copy.deepcopy(list(set(data_df.reset_index().Model.values)))\n",
    "            random.shuffle(systems)\n",
    "            df = data_df[data_df.index.isin(systems, level=0)]\n",
    "            df = df.reindex(systems)\n",
    "            ref_ranks = {\n",
    "             'mean':mean_aggregation_task_level(df)[0][:number_of_systems],\n",
    "            'direct': ranking_aggregation(df)[:number_of_systems],\n",
    "                    }\n",
    "            selected_systems = systems[:number_of_systems]\n",
    "            running_df = df[df.index.isin(selected_systems, level=0)]\n",
    "            running_df_new = running_df.reindex(selected_systems)\n",
    "\n",
    "\n",
    "            runing_ranks = {\n",
    "             'mean':mean_aggregation_task_level(running_df)[0],\n",
    "            'direct': ranking_aggregation(running_df)\n",
    "                    }\n",
    "\n",
    "            final_dic = {}\n",
    "\n",
    "            for r_key in ref_ranks.keys():\n",
    "                    final_dic['{}'.format(r_key)] = stats.kendalltau(ref_ranks[r_key], runing_ranks[r_key])[0]\n",
    "            l_final_dic.append(final_dic)  \n",
    "    return l_final_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATE_DATA_ROBUTNESS_SYSTEM  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if GENERATE_DATA_ROBUTNESS_SYSTEM:\n",
    "    for ds in DATASETS: \n",
    "        number_of_samples = 15\n",
    "        l_final_dic = generate_system_experiment_benchmark(number_of_samples,path= 'final_df',ds=ds,seed=42) \n",
    "        df_dic = {\n",
    "                'type':[],\n",
    "                'correlation':[],\n",
    "                'nbs':[]\n",
    "            }\n",
    "        for index,c_dic in enumerate(l_final_dic):\n",
    "            index_runing=index // number_of_samples \n",
    "            for k,v in c_dic.items():\n",
    "                    df_dic['nbs'].append(index_runing)\n",
    "                    df_dic['type'].append(k)\n",
    "                    df_dic['correlation'].append(v)\n",
    "        with open('final_dic_ajout_de_systems_{}_{}_benchmarks.json'.format(ds,number_of_samples),'w') as file:\n",
    "            json.dump(l_final_dic,file)\n",
    "        plt.figure(figsize=(12,8))\n",
    "        df_to_plot = pd.DataFrame(df_dic)\n",
    "        sns.lineplot(x=\"nbs\", y=\"correlation\", hue=\"type\",data=df_to_plot)\n",
    "        plt.title('{}'.format(ds))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "number_of_samples = 15\n",
    "for ds in DATASETS: \n",
    "        df_dic = {\n",
    "                'type':[],\n",
    "                'correlation':[],\n",
    "                'nbs':[]\n",
    "            }\n",
    "        with open('final_dic_ajout_de_systems_{}_{}_benchmarks.json'.format(ds,number_of_samples),'r') as file:\n",
    "            l_final_dic = json.load(file)\n",
    "        for index,c_dic in enumerate(l_final_dic):\n",
    "            index_runing=index // number_of_samples \n",
    "            for k,v in c_dic.items():\n",
    "                    df_dic['nbs'].append(index_runing)\n",
    "                    df_dic['type'].append(k)\n",
    "                    df_dic['correlation'].append(v)\n",
    "        plt.figure(figsize=(12,8))\n",
    "        df_to_plot = pd.DataFrame(df_dic)\n",
    "        sns.lineplot(x=\"nbs\", y=\"correlation\", hue=\"type\",data=df_to_plot)\n",
    "        plt.title('{}'.format(ds))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantitative analysis via agrements/kendall correlation/kendall distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyse_agreements_benchmark(data_path, type_of_agreement=1):\n",
    "    final_to_plot = {\n",
    "       'ds':[],\n",
    "        'is_1':[], #is equal\n",
    "        'is_3':[],\n",
    "        'is_5':[],\n",
    "        'is_10':[],\n",
    "        '1':[],\n",
    "        '3':[],\n",
    "        '5':[],\n",
    "        '10':[],\n",
    "        'k_1':[],\n",
    "        'k_3':[],\n",
    "        'k_5':[],\n",
    "        'k_10':[],\n",
    "        'tau':[],\n",
    "        'all':[],\n",
    "        'method':[],\n",
    "        }\n",
    "    method = {'0':1}\n",
    "    for ds in tqdm(DATASETS,'Datasets'):\n",
    "        assert type_of_agreement in [1,-1]\n",
    "        df =  pd.read_csv('data/{}.data'.format(ds),index_col=False)\n",
    "        considered_df =  pd.read_csv('data/{}.data'.format(ds),index_col=False,usecols = [i for i in range(df.shape[1]) if i!=1])\n",
    "       \n",
    "        mean_ranking,means = mean_aggregation_task_level(considered_df)\n",
    "        mean_reindex = np.argsort(-np.array(mean_ranking))\n",
    "        df_new = df.reindex(mean_reindex,copy = False)\n",
    "        df_new.reset_index(inplace=True)\n",
    "        del df_new['index']\n",
    "        # create rankings\n",
    "        mean_ranking = mean_aggregation_task_level(df_new)[0]\n",
    "        mean_aggreg = np.argsort(-np.array(mean_ranking))\n",
    "        ranking_ranking  = ranking_aggregation(df_new)\n",
    "        #ranking_ranking = [ranking_ranking.tolist().index(i) for i in range(len(ranking_ranking))]\n",
    "        \n",
    "        direct_aggreg = np.argsort(-np.array(ranking_ranking))\n",
    "        #direct_aggreg = [direct_aggreg.tolist().index(i) for i in range(len(direct_aggreg))]\n",
    "        \n",
    "        comparizons = [(mean_aggreg,direct_aggreg)]\n",
    "        print('Ds',ds)\n",
    "        print('mean_ranking',mean_ranking)\n",
    "        print('mean_aggreg',mean_aggreg)\n",
    "        \n",
    "        print('borda_ranking',ranking_ranking)\n",
    "        print('borda_aggreg',direct_aggreg)\n",
    "        for index, (a,b) in enumerate(comparizons):\n",
    "            final_to_plot['ds'].append(ds)\n",
    "            final_to_plot['method'].append(method[str(index)])\n",
    "            if type_of_agreement == 1: #best\n",
    "                final_to_plot['k_1'].append(stats.kendalltau(a[-1:], b[-1:])[0])\n",
    "                final_to_plot['k_3'].append(stats.kendalltau(a[-3:], b[-3:])[0])\n",
    "                final_to_plot['k_5'].append(stats.kendalltau(a[-5:], b[-5:])[0])\n",
    "                final_to_plot['k_10'].append(stats.kendalltau(a[-10:], b[-10:])[0])\n",
    "                final_to_plot['all'].append(stats.kendalltau(a, b)[0])\n",
    "                final_to_plot['1'].append(pu.distance(a[-1:], b[-1:]))\n",
    "                final_to_plot['3'].append(pu.distance(a[-3:], b[-3:]))\n",
    "                final_to_plot['5'].append(pu.distance(a[-5:], b[-5:]))\n",
    "                final_to_plot['10'].append(pu.distance(a[-10:], b[-10:]))\n",
    "                final_to_plot['is_1'].append(a[-1: ] == b[-1:])\n",
    "                final_to_plot['is_3'].append(a[-3:] == b[-3:])\n",
    "                final_to_plot['is_5'].append(a[-5:] == b[-5:])\n",
    "                final_to_plot['tau'].append(stats.kendalltau(a, b)[0])\n",
    "                final_to_plot['is_10'].append(a[-10:] == b[-10:])\n",
    "\n",
    "            else : #worst\n",
    "                final_to_plot['k_1'].append(stats.kendalltau(a[:1], b[:1])[0])\n",
    "                final_to_plot['k_3'].append(stats.kendalltau(a[:3], b[:3])[0])\n",
    "                final_to_plot['k_5'].append(stats.kendalltau(a[:5], b[:5])[0])\n",
    "                final_to_plot['k_10'].append(stats.kendalltau(a[:10], b[:10])[0])\n",
    "                final_to_plot['tau'].append(stats.kendalltau(a, b)[0])\n",
    "                final_to_plot['all'].append(stats.kendalltau(a, b)[0])\n",
    "                final_to_plot['1'].append(pu.distance(a[:1], b[:1]))\n",
    "                final_to_plot['3'].append(pu.distance(a[:3], b[:3]))\n",
    "                final_to_plot['5'].append(pu.distance(a[:5], b[:5]))\n",
    "                final_to_plot['10'].append(pu.distance(a[:10], b[:10]))  \n",
    "                final_to_plot['is_1'].append(a[0] == b[0])\n",
    "                final_to_plot['is_3'].append(a[:3] == b[:3])\n",
    "                final_to_plot['is_5'].append(a[:5] == b[:5])\n",
    "                final_to_plot['is_10'].append(a[:10] == b[:10])\n",
    "    return final_to_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_to_plot=  analyse_agreements_benchmark(data_path = 'final_df', type_of_agreement=-1)  \n",
    "data_df = pd.DataFrame(final_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_to_plot=  analyse_agreements_benchmark(data_path = 'final_df', type_of_agreement=1)  \n",
    "data_df =pd.DataFrame(final_to_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Display with barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_to_plot=  analyse_agreements_benchmark(data_path = 'final_df', type_of_agreement=-1)  \n",
    "data_df =pd.DataFrame(final_to_plot)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = 'k_1', y = 'ds', hue = 'method', data = data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =pd.DataFrame(final_to_plot)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = 'k_3', y = 'ds', hue = 'method', data = data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =pd.DataFrame(final_to_plot)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = 'k_5', y = 'ds', hue = 'method', data = data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =pd.DataFrame(final_to_plot)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = 'k_10', y = 'ds', hue = 'method', data = data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df =pd.DataFrame(final_to_plot)\n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x = 'is_1', y = 'ds', hue = 'method', data = data_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
